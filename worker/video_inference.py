"""
Wan 2.1 Video Generation with NSFW support.

Supports:
- Text-to-Video (t2v)
- Image-to-Video (i2v)
- LoRA loading for character/style control
"""
import uuid
import os
import gc
import torch
from PIL import Image
from r2_client import download, upload

# Global pipeline caches
_wan_t2v_pipeline = None
_wan_i2v_pipeline = None
_loaded_loras = []

# Wan model configuration
# For NSFW T2V: Use NSFW-API models (1.3B is faster for RunPod 24GB VRAM)
# For I2V: Use standard Wan model (NSFW I2V models are limited)
# The input image itself can be NSFW (generated by SD with LoRAs)
WAN_MODEL_T2V = os.environ.get("WAN_MODEL_T2V", "NSFW-API/NSFW_Wan_1.3b")
WAN_MODEL_I2V = os.environ.get("WAN_MODEL_I2V", "Wan-Video/Wan2.1-I2V-1.3B-480P")

# LoRA registry - can store LoRAs in R2
# Video LoRA registry - Wan 2.1/2.2 compatible video LoRAs stored in R2
# Run scripts/download_video_loras.py to populate these in R2
LORA_REGISTRY = {
    # === GENERAL NSFW (CubeyAI) ===
    # Trigger: "nsfwsks" + descriptive prompting
    "nsfw-22-high": "video-loras/NSFW-22-H-e8.safetensors",
    "nsfw-22-low": "video-loras/NSFW-22-L-e8.safetensors",
    "nsfw-21": "video-loras/wan-nsfw-e14-fixed.safetensors",
    
    # === CUMSHOT LoRAs ===
    # Trigger: "cum shoots out of the man's penis and lands on her face/chest"
    "cumshot-22-high": "video-loras/wan2.2_highnoise_cumshot_v.1.0.safetensors",
    "cumshot-22-low": "video-loras/wan2.2_lownoise_cumshot_v1.0.safetensors",
    "cumshot-21-t2v": "video-loras/wan_cumshot.safetensors",
    "cumshot-21-i2v": "video-loras/wan_cumshot_i2v.safetensors",
    
    # === FACIAL / BUKKAKE ===
    # Trigger: "thick whitish translucent semen, cum on face"
    "facial-21": "video-loras/facials_epoch_50.safetensors",
}


def clear_memory():
    """Clear GPU memory."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def get_wan_t2v_pipeline():
    """Load or reuse the Wan 2.1 Text-to-Video pipeline."""
    global _wan_t2v_pipeline, _wan_i2v_pipeline

    if _wan_t2v_pipeline is not None:
        return _wan_t2v_pipeline

    # Free I2V pipeline if loaded to save VRAM
    if _wan_i2v_pipeline is not None:
        print("Unloading I2V pipeline to load T2V...")
        del _wan_i2v_pipeline
        _wan_i2v_pipeline = None
        clear_memory()

    print(f"Loading Wan 2.1 T2V pipeline: {WAN_MODEL_T2V}")

    from diffusers import WanPipeline

    _wan_t2v_pipeline = WanPipeline.from_pretrained(
        WAN_MODEL_T2V,
        torch_dtype=torch.float16,
    )
    _wan_t2v_pipeline.to("cuda")

    # Enable memory optimizations
    _wan_t2v_pipeline.enable_model_cpu_offload()

    print("Wan T2V pipeline loaded!")
    return _wan_t2v_pipeline


def get_wan_i2v_pipeline():
    """Load or reuse the Wan 2.1 Image-to-Video pipeline."""
    global _wan_i2v_pipeline, _wan_t2v_pipeline

    if _wan_i2v_pipeline is not None:
        return _wan_i2v_pipeline

    # Free T2V pipeline if loaded to save VRAM
    if _wan_t2v_pipeline is not None:
        print("Unloading T2V pipeline to load I2V...")
        del _wan_t2v_pipeline
        _wan_t2v_pipeline = None
        clear_memory()

    print(f"Loading Wan 2.1 I2V pipeline: {WAN_MODEL_I2V}")

    from diffusers import WanImageToVideoPipeline

    _wan_i2v_pipeline = WanImageToVideoPipeline.from_pretrained(
        WAN_MODEL_I2V,
        torch_dtype=torch.float16,
    )
    _wan_i2v_pipeline.to("cuda")

    # Enable memory optimizations
    _wan_i2v_pipeline.enable_model_cpu_offload()

    print("Wan I2V pipeline loaded!")
    return _wan_i2v_pipeline


def load_loras(pipe, lora_names):
    """Load LoRAs onto the pipeline."""
    global _loaded_loras

    for lora_name in lora_names:
        if lora_name in _loaded_loras:
            continue

        if lora_name in LORA_REGISTRY:
            lora_r2_key = LORA_REGISTRY[lora_name]
            local_lora = f"/tmp/loras/{lora_name}.safetensors"
            os.makedirs("/tmp/loras", exist_ok=True)

            print(f"Downloading LoRA: {lora_name}")
            download(lora_r2_key, local_lora)

            print(f"Loading LoRA: {lora_name}")
            pipe.load_lora_weights(local_lora, adapter_name=lora_name)
            _loaded_loras.append(lora_name)
        else:
            print(f"Warning: LoRA '{lora_name}' not found in registry")

    # Set active adapters
    if _loaded_loras:
        pipe.set_adapters(_loaded_loras)


def export_video(frames, output_path, fps=8):
    """Export frames to MP4 video."""
    import imageio

    # Convert tensor frames to numpy if needed
    if hasattr(frames, 'cpu'):
        frames = frames.cpu().numpy()

    # Ensure frames are in correct format (T, H, W, C) with values 0-255
    if frames.max() <= 1.0:
        frames = (frames * 255).astype('uint8')

    imageio.mimwrite(output_path, frames, fps=fps, codec='libx264', quality=8)
    print(f"Video saved: {output_path}")


def run_video_inference(
    job_id,
    user_id,
    input_keys,
    output_prefix,
    params,
):
    """
    Generate video using Wan 2.1.

    If input_keys is provided: Image-to-Video
    If only prompt is provided: Text-to-Video
    """
    outputs = []

    # Get parameters
    prompt = params.get("prompt", "")
    negative_prompt = params.get("negative_prompt", "blurry, low quality, distorted, watermark")
    num_frames = params.get("num_frames", 16)  # Wan default
    fps = params.get("fps", 8)
    guidance_scale = params.get("guidance_scale", 5.0)
    num_inference_steps = params.get("num_inference_steps", 30)
    width = params.get("width", 480)
    height = params.get("height", 720)
    seed = params.get("seed", None)
    lora_names = params.get("lora_names", [])

    # Set up generator
    generator = None
    if seed is not None:
        generator = torch.Generator(device="cuda").manual_seed(seed)
    else:
        generator = torch.Generator(device="cuda").manual_seed(torch.randint(0, 2**32, (1,)).item())

    # Decide mode: T2V or I2V
    if input_keys and len(input_keys) > 0:
        # Image-to-Video mode
        print(f"Running Image-to-Video with {len(input_keys)} input(s)")
        pipe = get_wan_i2v_pipeline()

        # Load LoRAs if specified
        if lora_names:
            load_loras(pipe, lora_names)

        for key in input_keys:
            local_input = f"/tmp/{uuid.uuid4()}.png"
            local_output = f"/tmp/{uuid.uuid4()}.mp4"

            # Download input image
            download(key, local_input)

            # Load and prepare image
            image = Image.open(local_input).convert("RGB")

            # Resize to target dimensions
            image = image.resize((width, height), Image.LANCZOS)

            print(f"Generating I2V: {num_frames} frames at {width}x{height}")
            print(f"Prompt: {prompt}")

            # Generate video
            output = pipe(
                image=image,
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_frames=num_frames,
                guidance_scale=guidance_scale,
                num_inference_steps=num_inference_steps,
                generator=generator,
            )

            # Export video
            frames = output.frames[0]
            export_video(frames, local_output, fps=fps)

            # Upload to R2
            output_key = f"{output_prefix}{uuid.uuid4()}.mp4"
            upload(local_output, output_key)

            outputs.append({
                "key": output_key,
                "type": "video",
                "mode": "i2v",
                "fps": fps,
                "num_frames": num_frames,
            })

            # Cleanup
            os.remove(local_input)
            os.remove(local_output)

            print(f"I2V video generated: {output_key}")

    else:
        # Text-to-Video mode
        print(f"Running Text-to-Video")
        pipe = get_wan_t2v_pipeline()

        # Load LoRAs if specified
        if lora_names:
            load_loras(pipe, lora_names)

        local_output = f"/tmp/{uuid.uuid4()}.mp4"

        print(f"Generating T2V: {num_frames} frames at {width}x{height}")
        print(f"Prompt: {prompt}")

        # Generate video
        output = pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_frames=num_frames,
            width=width,
            height=height,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            generator=generator,
        )

        # Export video
        frames = output.frames[0]
        export_video(frames, local_output, fps=fps)

        # Upload to R2
        output_key = f"{output_prefix}{uuid.uuid4()}.mp4"
        upload(local_output, output_key)

        outputs.append({
            "key": output_key,
            "type": "video",
            "mode": "t2v",
            "fps": fps,
            "num_frames": num_frames,
        })

        # Cleanup
        os.remove(local_output)

        print(f"T2V video generated: {output_key}")

    return outputs

